{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "estudo_spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/818UEZVJZO4p2fTZrxzj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leandro-Braga/Estudo_de_Caso/blob/main/estudo_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MRyp0NMNfzMz"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "3LLZFXzof1hY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "ibedhHA4f1r9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "iF9VNiY-f1zx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzq_aRu0f18H",
        "outputId": "74d39105-bc26-431d-ba8c-c6c931ac8679"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 38.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME'] = '/content/spark-3.1.2-bin-hadoop3.2'"
      ],
      "metadata": {
        "id": "v8n3m443f2EH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Bzs9BSEvgQHy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "ReWFl_nugQTK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lendo um dataset já exitente no spark"
      ],
      "metadata": {
        "id": "CwwSLpCIhiNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv',inferSchema=True, header =True)"
      ],
      "metadata": {
        "id": "yADJ8ATCgvPx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibir o dataset no spark de exemplo"
      ],
      "metadata": {
        "id": "hnPqNI2xhyRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpkglNVKgvZ5",
        "outputId": "24e8bd71-df1b-4b57-945d-eb184fbe8f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalizar a seção do spark"
      ],
      "metadata": {
        "id": "5IgJETTBh8xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "ZJdfKqiAgvhJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cluster Spark do PySpark e instância da classe SparkContext"
      ],
      "metadata": {
        "id": "1UOfZdHGj_hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext() \n",
        "print(spark_contexto)           \n",
        "print(spark_contexto.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDhphK52gvpD",
        "outputId": "f86822e7-51ff-44e0-dca3-ea5eba9365ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "criar uma seção no Spark (SparkSession)"
      ],
      "metadata": {
        "id": "sXOfk1rWj5vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession \n",
        "spark = SparkSession.builder.getOrCreate() # Create my_spark\n",
        "print(spark) # Print my_spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiwqXQQPgvvx",
        "outputId": "db1b22fb-9e06-4b18-ad9c-ca2309907314"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7fb4bddfb890>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv',inferSchema=True, header =True)"
      ],
      "metadata": {
        "id": "P8NFSM0XkOQz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_-VONwnkTeM",
        "outputId": "8cf4bec5-039a-40f3-d1d2-69e2b24e55aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdnXjC9vkTVJ",
        "outputId": "d69c92d4-e8ce-4a55-db5f-452050640c3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.createOrReplaceTempView('tabela_temporaria')\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V3obJQqkTMA",
        "outputId": "5f2a7194-a797-47ad-e2c7-599410a2ced5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'FROM tabela_temporaria SELECT longitude, latitude LIMIT 3'  \n",
        "saida = spark.sql(query)  \n",
        "saida.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUwxj2V7kTBQ",
        "outputId": "d1161b2a-f0e4-47ae-ae83-e493af66e374"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|longitude|latitude|\n",
            "+---------+--------+\n",
            "|  -122.05|   37.37|\n",
            "|   -118.3|   34.26|\n",
            "|  -117.81|   33.78|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converter o DataFrame do Spark para o DataFrame do Pandas."
      ],
      "metadata": {
        "id": "OT10Qq4rnHnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = 'SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria'\n",
        "q_maximo_quartos = spark.sql(query1)\n",
        "pd_maximo_quartos = q_maximo_quartos.toPandas()\n",
        "print('A quantidade máxima de quartos é: {}'.format(pd_maximo_quartos['maximo_quartos']))\n",
        "qtd_maximo_quartos = int(pd_maximo_quartos.loc[0,'maximo_quartos'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8TvQNXVlnL4",
        "outputId": "0bb1d09e-b18b-4111-ae01-2fe9477957e5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A quantidade máxima de quartos é: 0    30450.0\n",
            "Name: maximo_quartos, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executar o SQL no Spark e obter o resultado no DataFrame do Spark."
      ],
      "metadata": {
        "id": "b9d8-2DfnKN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = f'SELECT longitude, latitude FROM tabela_temporaria WHERE total_rooms = {str(qtd_maximo_quartos)}'\n",
        "localizacao_maximo_quartos = spark.sql(query2)\n",
        "pd_localizacao_maximo_quartos = localizacao_maximo_quartos.toPandas()\n",
        "print(pd_localizacao_maximo_quartos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS6tCzxXlnGC",
        "outputId": "6ebf1331-3862-475a-84ce-a60e2fedbfb4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude\n",
            "0     -117.2     33.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exemplo que converte um DataFrame do Pandas para um DataFrame no Spark"
      ],
      "metadata": {
        "id": "4tRVJ05SpDh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "media = 0\n",
        "desvio_padrao=0.1 \n",
        "pd_temporario = pd.DataFrame(np.random.normal(media,desvio_padrao,100))\n",
        "spark_temporario = spark.createDataFrame(pd_temporario)\n",
        "print(spark.catalog.listTables())\n",
        "spark_temporario.createOrReplaceTempView('nova_tabela_temporaria')\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhP7gqo6lm-U",
        "outputId": "5a8d4dc2-6f58-4640-a3f2-33a6f16a5f03"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finalizar o spark"
      ],
      "metadata": {
        "id": "orjZoUxxp-lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "70-sTA00lm3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "stMCPQMflmul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}