{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "estudo_spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2WZbGsClPLMZz/0wpK/ow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leandro-Braga/Estudo_de_Caso/blob/main/estudo_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MRyp0NMNfzMz"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "3LLZFXzof1hY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "ibedhHA4f1r9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "iF9VNiY-f1zx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzq_aRu0f18H",
        "outputId": "74d39105-bc26-431d-ba8c-c6c931ac8679"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 38.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME'] = '/content/spark-3.1.2-bin-hadoop3.2'"
      ],
      "metadata": {
        "id": "v8n3m443f2EH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Bzs9BSEvgQHy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "ReWFl_nugQTK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lendo um dataset já exitente no spark"
      ],
      "metadata": {
        "id": "CwwSLpCIhiNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv',inferSchema=True, header =True)"
      ],
      "metadata": {
        "id": "yADJ8ATCgvPx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exibir o dataset no spark de exemplo"
      ],
      "metadata": {
        "id": "hnPqNI2xhyRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpkglNVKgvZ5",
        "outputId": "24e8bd71-df1b-4b57-945d-eb184fbe8f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalizar a seção do spark"
      ],
      "metadata": {
        "id": "5IgJETTBh8xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "ZJdfKqiAgvhJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cluster Spark do PySpark e instância da classe SparkContext"
      ],
      "metadata": {
        "id": "1UOfZdHGj_hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext() \n",
        "print(spark_contexto)           \n",
        "print(spark_contexto.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDhphK52gvpD",
        "outputId": "f86822e7-51ff-44e0-dca3-ea5eba9365ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "criar uma seção no Spark (SparkSession)"
      ],
      "metadata": {
        "id": "sXOfk1rWj5vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession \n",
        "spark = SparkSession.builder.getOrCreate() # Create my_spark\n",
        "print(spark) # Print my_spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiwqXQQPgvvx",
        "outputId": "db1b22fb-9e06-4b18-ad9c-ca2309907314"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7fb4bddfb890>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv',inferSchema=True, header =True)"
      ],
      "metadata": {
        "id": "P8NFSM0XkOQz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_-VONwnkTeM",
        "outputId": "8cf4bec5-039a-40f3-d1d2-69e2b24e55aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdnXjC9vkTVJ",
        "outputId": "d69c92d4-e8ce-4a55-db5f-452050640c3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.createOrReplaceTempView('tabela_temporaria')\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V3obJQqkTMA",
        "outputId": "5f2a7194-a797-47ad-e2c7-599410a2ced5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'FROM tabela_temporaria SELECT longitude, latitude LIMIT 3'  \n",
        "saida = spark.sql(query)  \n",
        "saida.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUwxj2V7kTBQ",
        "outputId": "d1161b2a-f0e4-47ae-ae83-e493af66e374"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|longitude|latitude|\n",
            "+---------+--------+\n",
            "|  -122.05|   37.37|\n",
            "|   -118.3|   34.26|\n",
            "|  -117.81|   33.78|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converter o DataFrame do Spark para o DataFrame do Pandas."
      ],
      "metadata": {
        "id": "OT10Qq4rnHnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = 'SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria'\n",
        "q_maximo_quartos = spark.sql(query1)\n",
        "pd_maximo_quartos = q_maximo_quartos.toPandas()\n",
        "print('A quantidade máxima de quartos é: {}'.format(pd_maximo_quartos['maximo_quartos']))\n",
        "qtd_maximo_quartos = int(pd_maximo_quartos.loc[0,'maximo_quartos'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8TvQNXVlnL4",
        "outputId": "0bb1d09e-b18b-4111-ae01-2fe9477957e5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A quantidade máxima de quartos é: 0    30450.0\n",
            "Name: maximo_quartos, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executar o SQL no Spark e obter o resultado no DataFrame do Spark."
      ],
      "metadata": {
        "id": "b9d8-2DfnKN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = f'SELECT longitude, latitude FROM tabela_temporaria WHERE total_rooms = {str(qtd_maximo_quartos)}'\n",
        "localizacao_maximo_quartos = spark.sql(query2)\n",
        "pd_localizacao_maximo_quartos = localizacao_maximo_quartos.toPandas()\n",
        "print(pd_localizacao_maximo_quartos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS6tCzxXlnGC",
        "outputId": "6ebf1331-3862-475a-84ce-a60e2fedbfb4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude\n",
            "0     -117.2     33.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exemplo que converte um DataFrame do Pandas para um DataFrame no Spark"
      ],
      "metadata": {
        "id": "4tRVJ05SpDh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "media = 0\n",
        "desvio_padrao=0.1 \n",
        "pd_temporario = pd.DataFrame(np.random.normal(media,desvio_padrao,100))\n",
        "spark_temporario = spark.createDataFrame(pd_temporario)\n",
        "print(spark.catalog.listTables())\n",
        "spark_temporario.createOrReplaceTempView('nova_tabela_temporaria')\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhP7gqo6lm-U",
        "outputId": "5a8d4dc2-6f58-4640-a3f2-33a6f16a5f03"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "finalizar o spark"
      ],
      "metadata": {
        "id": "orjZoUxxp-lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "70-sTA00lm3B"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MapReduce"
      ],
      "metadata": {
        "id": "3jtdkEh2tXkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext()"
      ],
      "metadata": {
        "id": "stMCPQMflmul"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vetor = np.array([10, 20, 30, 40, 50])"
      ],
      "metadata": {
        "id": "8yodnLQztl-b"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos criar um RDD por meio de um SparkContext com o seguinte código"
      ],
      "metadata": {
        "id": "VwNHGcBDtz_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_contexto.parallelize(vetor)"
      ],
      "metadata": {
        "id": "q5OVtd7Rtl5c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paralelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQEW_yiVtlz6",
        "outputId": "7798daf0-eb4b-463d-e9c5-d4711d497376"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa saída indica que criamos o RDD com sucesso. Logo, o vetor já está no Spark e podemos aplicar o Mapeamento."
      ],
      "metadata": {
        "id": "Jd9GXtT-t8BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapa = paralelo.map(lambda x : x**2+x)"
      ],
      "metadata": {
        "id": "pjftOwietltv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lambda x : x**2+x é uma função lambda que corresponde à função matemática que queremos aplicar aos elementos da entrada de dados. O código paralelo.map faz o mapeamento da função para cada elemento da entrada."
      ],
      "metadata": {
        "id": "NYmHcMg9uCJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapa.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C0kh5idtlme",
        "outputId": "4a3066ff-dda8-4000-9345-9c706cb63ecb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 420, 930, 1640, 2550]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse exemplo, o objetivo é contar a frequência das palavras de uma lista. Então, o primeiro passo é entrar com uma lista de palavras, conforme o código abaixo:"
      ],
      "metadata": {
        "id": "N03oz0pWuKiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_contexto.parallelize(['distribuida', 'distribuida', 'spark', 'rdd', 'spark','spark'])"
      ],
      "metadata": {
        "id": "G9uhqi5stlXO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com isso, a variável “paralelo” faz referência ao RDD. O próximo passo é implementar uma função lambda que simplesmente associa o número “1” a uma palavra."
      ],
      "metadata": {
        "id": "MeHD-SWeuO-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "funcao_lambda = lambda x:(x,1)\n",
        "\n",
        "# a função recebe uma variável “x” e vai produzir o par (x, 1)."
      ],
      "metadata": {
        "id": "vzBKaRV4uPey"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  vamos aplicar o MapReduce\n",
        "from operator import add\n",
        "mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()\n",
        "\n",
        "# Na primeira linha, importamos o operador “add” que será usado para somar as ocorrências das palavras mapeadas.\n",
        "# Já na segunda linha, muitas coisas estão ocorrendo:\n",
        "\"\"\" 1 Fazemos o mapeamento dos dados da variável “paralelo” para a função lambda, ou seja, para cada palavra criamos um par (palavra, 1).\n",
        "    2 Em seguida, aplicamos a redução com a função “reduceKey”, que soma as ocorrências e as agrupa pela chave, no caso, pelas palavras.\n",
        "    3 Na etapa final, dada pela função “collect”, fazemos a coleta dos dados em uma lista que chamamos de “mapa”.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ykg09NWGuQSC",
        "outputId": "3a14ed02-354e-432a-dd23-5719c4f95caa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 1 Fazemos o mapeamento dos dados da variável “paralelo” para a função lambda, ou seja, para cada palavra criamos um par (palavra, 1).\\n    2 Em seguida, aplicamos a redução com a função “reduceKey”, que soma as ocorrências e as agrupa pela chave, no caso, pelas palavras.\\n    3 Na etapa final, dada pela função “collect”, fazemos a coleta dos dados em uma lista que chamamos de “mapa”.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse código percorre a lista “mapa” e imprime cada par formado pela palavra e sua respectiva ocorrência."
      ],
      "metadata": {
        "id": "6eJEYLHbu51C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (w, c) in mapa:\n",
        "    print('{}: {}'.format(w, c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLMbJIzjuQMk",
        "outputId": "0a3fe6fc-731e-4e73-8f93-6e6c8f6f0116"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribuida: 2\n",
            "spark: 3\n",
            "rdd: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "HwZds1nsuQFP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "desenvolver um exemplo que envolve as operações de transformações e de ações."
      ],
      "metadata": {
        "id": "onUJdW_3x0qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext()"
      ],
      "metadata": {
        "id": "d8PnOnGAxzzl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista = [1, 2, 3, 4, 5, 3]\n",
        "lista_rdd = spark_contexto.parallelize(lista)"
      ],
      "metadata": {
        "id": "5PpLEpJGxzt9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  contar os elementos do RDD.\n",
        "lista_rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOstOw-Wxznd",
        "outputId": "0ad3283a-810c-46ed-e0ee-b7a5aa379dff"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vamos criar uma função lambda que recebe um número como parâmetro e retorna um par formador pelo número do parâmetro e pelo mesmo número multiplicado por 10"
      ],
      "metadata": {
        "id": "QO21u1a0yFFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "par_ordenado = lambda numero: (numero, numero*10)"
      ],
      "metadata": {
        "id": "9WNhH8rSxzgz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse próximo passo, vamos aplicar a transformação “flatMap” com a ação “collect” da função lambda para a “lista_rdd”:"
      ],
      "metadata": {
        "id": "c6eeYzObyIYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.flatMap(par_ordenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKLkqGmjxzZ4",
        "outputId": "1ce9b742-cf24-43b2-cff5-1d4a64ea2263"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 3, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vamos aplicar a transformação “map” com a ação “collect” da função lambda para a “lista_rdd”:"
      ],
      "metadata": {
        "id": "tjSWWYQFyYWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.map(par_ordenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfIJplvpyZ_q",
        "outputId": "e0472d53-f9c3-460b-a058-c244843f8c0a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 10), (2, 20), (3, 30), (4, 40), (5, 50), (3, 30)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "uor8IbDVyarQ"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}